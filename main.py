# main.py (ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ìƒˆë¡œìš´ ì‹œì‘ì )

import pandas as pd
import json
import re
from transformers import AutoTokenizer

# --- 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ---

print("--- [Phase 1] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì‹œì‘ ---")
# íŒŒì¼ ê²½ë¡œ ì„¤ì •
file_path = './data/'

# í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¡œë”© (ì´ì „ê³¼ ë™ì¼)
with open(file_path + 'training-label.json', 'r', encoding='utf-8') as file:
    training_data_raw = json.load(file)
with open(file_path + 'validation-label.json', 'r', encoding='utf-8') as file:
    validation_data_raw = json.load(file)

# DataFrame ìƒì„± í•¨ìˆ˜ (ì½”ë“œë¥¼ ê¹”ë”í•˜ê²Œ í•˜ê¸° ìœ„í•´ í•¨ìˆ˜ë¡œ ë¬¶ìŒ)
def create_dataframe(data_raw):
    extracted_data = []
    for dialogue in data_raw:
        try:
            emotion_type = dialogue['profile']['emotion']['type']
            dialogue_content = dialogue['talk']['content']
            full_text = " ".join(list(dialogue_content.values()))
            if full_text and emotion_type:
                extracted_data.append({'text': full_text, 'emotion': emotion_type})
        except KeyError:
            continue
    return pd.DataFrame(extracted_data)

df_train = create_dataframe(training_data_raw)
df_val = create_dataframe(validation_data_raw)

# í…ìŠ¤íŠ¸ ì •ì œ
def clean_text(text):
    return re.sub(r'[^ê°€-í£a-zA-Z0-9 ]', '', text)

df_train['cleaned_text'] = df_train['text'].apply(clean_text)
df_val['cleaned_text'] = df_val['text'].apply(clean_text)
print("âœ… ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ!")


# --- 2. AI ëª¨ë¸ë§ ì¤€ë¹„ ---
print("\n--- [Phase 2] AI ëª¨ë¸ë§ ì¤€ë¹„ ì‹œì‘ ---")
# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
MODEL_NAME = "klue/roberta-base"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# í…ìŠ¤íŠ¸ í† í°í™”
train_tokenized = tokenizer(list(df_train['cleaned_text']), return_tensors="pt", max_length=128, padding=True, truncation=True)
val_tokenized = tokenizer(list(df_val['cleaned_text']), return_tensors="pt", max_length=128, padding=True, truncation=True)

# ë¼ë²¨ ì¸ì½”ë”©
unique_labels = sorted(df_train['emotion'].unique())
label_to_id = {label: id for id, label in enumerate(unique_labels)}
id_to_label = {id: label for label, id in label_to_id.items()}
df_train['label'] = df_train['emotion'].map(label_to_id)
df_val['label'] = df_val['emotion'].map(label_to_id)
print("âœ… í† í°í™” ë° ë¼ë²¨ ì¸ì½”ë”© ì™„ë£Œ!")
print("ì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ëª¨ë“  ì¤€ë¹„ê°€ ëë‚¬ìŠµë‹ˆë‹¤.")


# -----------------------------------------------------------
# --- [Phase 3] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ---
# -----------------------------------------------------------

import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

print("\n--- [Phase 3] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ ì‹œì‘ ---")

# 1. PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜
# í† í°í™”ëœ ë°ì´í„°ì™€ ë¼ë²¨ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ PyTorchê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
class EmotionDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # input_ids, attention_mask ë“±ì„ ëª¨ë‘ ê°€ì ¸ì˜µë‹ˆë‹¤.
        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Dataset ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
train_dataset = EmotionDataset(train_tokenized, df_train['label'].tolist())
val_dataset = EmotionDataset(val_tokenized, df_val['label'].tolist())
print("âœ… PyTorch ë°ì´í„°ì…‹ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# 2. AI ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# AutoModelForSequenceClassificationì€ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œì— íŠ¹í™”ëœ klue/roberta-base ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=len(unique_labels), # ë¶„ë¥˜í•´ì•¼ í•  ê°ì •ì˜ ì¢…ë¥˜ ê°œìˆ˜
    id2label=id_to_label,           # ìˆ«ì ë¼ë²¨ì„ ê°ì • ì´ë¦„ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©
    label2id=label_to_id            # ê°ì • ì´ë¦„ì„ ìˆ«ì ë¼ë²¨ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©
)
# GPUê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPU ë©”ëª¨ë¦¬ì— ì˜¬ë¦½ë‹ˆë‹¤.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ëª¨ë¸ì€ {device}ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.")


# 3. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ë¥¼ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=0)
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# 4. í›ˆë ¨ì„ ìœ„í•œ ìƒì„¸ ì„¤ì •(Arguments) ì •ì˜
training_args = TrainingArguments(
    output_dir='./results',          # í›ˆë ¨ ê²°ê³¼ë¬¼(ëª¨ë¸ ë“±)ì´ ì €ì¥ë  ë””ë ‰í† ë¦¬
    num_train_epochs=3,              # ì „ì²´ ë°ì´í„°ë¥¼ ì´ 3ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµ
    per_device_train_batch_size=16,  # í•œ ë²ˆì— GPUì— ì˜¬ë¦´ í›ˆë ¨ ë°ì´í„° ê°œìˆ˜ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8ë¡œ ì¤„ì´ê¸°)
    per_device_eval_batch_size=64,   # í•œ ë²ˆì— GPUì— ì˜¬ë¦´ í‰ê°€ ë°ì´í„° ê°œìˆ˜
    warmup_steps=500,                # ì´ˆê¸° í•™ìŠµë¥ ì„ ì ì§„ì ìœ¼ë¡œ ì˜¬ë¦¬ëŠ” ë‹¨ê³„ ìˆ˜
    weight_decay=0.01,               # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê°€ì¤‘ì¹˜ ê°ì†Œ
    logging_dir='./logs',            # ë¡œê·¸ê°€ ì €ì¥ë  ë””ë ‰í† ë¦¬
    logging_steps=500,               # 500 ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸(í•™ìŠµ í˜„í™©) ì¶œë ¥
    evaluation_strategy="steps",     # 500 ìŠ¤í…ë§ˆë‹¤ í‰ê°€ ì‹¤í–‰
    eval_steps=500,                  # í‰ê°€ ì‹¤í–‰ ì£¼ê¸°
    save_strategy="steps",           # 500 ìŠ¤í…ë§ˆë‹¤ ëª¨ë¸ ì €ì¥
    save_steps=500,
    load_best_model_at_end=True,     # í›ˆë ¨ ì¢…ë£Œ í›„ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ë˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜´
    report_to="none"                 # ì™¸ë¶€ ì„œë¹„ìŠ¤(wandb ë“±)ì— ë¡œê·¸ ì „ì†¡ ì•ˆ í•¨
)

# 5. Trainer ì •ì˜
# ëª¨ë¸, í›ˆë ¨ ì„¤ì •, í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹, í‰ê°€ í•¨ìˆ˜ë¥¼ ëª¨ë‘ íŠ¸ë ˆì´ë„ˆì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# 6. ëª¨ë¸ í›ˆë ¨ ì‹œì‘!
print("\nğŸ”¥ AI ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤... (RTX 4060 ê¸°ì¤€ ì•½ 30ë¶„~1ì‹œê°„ ì†Œìš” ì˜ˆìƒ)")
trainer.train()

print("\nğŸ‰ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")

# 7. ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
print("\n--- ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ---")
final_evaluation = trainer.evaluate()
print(final_evaluation)

print("\nëª¨ë“  ê³¼ì •ì´ ì„±ê³µì ìœ¼ë¡œ ëë‚¬ìŠµë‹ˆë‹¤! results í´ë”ì—ì„œ í›ˆë ¨ëœ ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")